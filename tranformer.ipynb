{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "final-mt-transformer-1-kor-viet.ipynb",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Kor - Viet\n",
        "https://pytorch.org/tutorials/beginner/translation_transformer.html"
      ],
      "metadata": {
        "id": "peDlHfRw4MGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dataset"
      ],
      "metadata": {
        "id": "AMUIJV3GIvqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv('../input/train-dev-test-kor-vi-mt/train.csv')\n",
        "dev = pd.read_csv('../input/train-dev-test-kor-vi-mt/dev.csv')\n",
        "test = pd.read_csv('../input/train-dev-test-kor-vi-mt/test.csv')"
      ],
      "metadata": {
        "id": "vyMk8IBAnaFC",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:12:31.178655Z",
          "iopub.execute_input": "2021-12-19T16:12:31.178953Z",
          "iopub.status.idle": "2021-12-19T16:12:34.606138Z",
          "shell.execute_reply.started": "2021-12-19T16:12:31.178923Z",
          "shell.execute_reply": "2021-12-19T16:12:34.605177Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "slp0Ubv5iKPi",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:12:34.608088Z",
          "iopub.execute_input": "2021-12-19T16:12:34.608416Z",
          "iopub.status.idle": "2021-12-19T16:12:34.637774Z",
          "shell.execute_reply.started": "2021-12-19T16:12:34.608379Z",
          "shell.execute_reply": "2021-12-19T16:12:34.636656Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare dataset to Dataloader:\n",
        "https://towardsdatascience.com/custom-datasets-in-pytorch-part-2-text-machine-translation-71c41a3e994e"
      ],
      "metadata": {
        "id": "g0FaA-4bfaie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandarallel"
      ],
      "metadata": {
        "id": "vS9IFwRBfmtu",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:12:34.639564Z",
          "iopub.execute_input": "2021-12-19T16:12:34.639936Z",
          "iopub.status.idle": "2021-12-19T16:12:45.599813Z",
          "shell.execute_reply.started": "2021-12-19T16:12:34.639893Z",
          "shell.execute_reply": "2021-12-19T16:12:45.598699Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sys libs\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#data manupulation libs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pandarallel import pandarallel\n",
        "# Initialization\n",
        "pandarallel.initialize()\n",
        "\n",
        "\n",
        "#string manupulation libs\n",
        "import re\n",
        "import string\n",
        "from string import digits\n",
        "import spacy\n",
        "\n",
        "#torch libs\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "yEDufKPufdwh",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:12:45.603327Z",
          "iopub.execute_input": "2021-12-19T16:12:45.603635Z",
          "iopub.status.idle": "2021-12-19T16:12:55.467533Z",
          "shell.execute_reply.started": "2021-12-19T16:12:45.603601Z",
          "shell.execute_reply": "2021-12-19T16:12:55.466447Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "  \n",
        "    '''\n",
        "    __init__ method is called by default as soon as an object of this class is initiated\n",
        "    we use this method to initiate our vocab dictionaries\n",
        "    '''\n",
        "    def __init__(self, freq_threshold, max_size):\n",
        "        '''\n",
        "        freq_threshold : the minimum times a word must occur in corpus to be treated in vocab\n",
        "        max_size : max source vocab size. Eg. if set to 10,000, we pick the top 10,000 most frequent words and discard others\n",
        "        '''\n",
        "        #initiate the index to token dict\n",
        "        ## <PAD> -> padding, used for padding the shorter sentences in a batch to match the length of longest sentence in the batch\n",
        "        ## <SOS> -> start token, added in front of each sentence to signify the start of sentence\n",
        "        ## <EOS> -> End of sentence token, added to the end of each sentence to signify the end of sentence\n",
        "        ## <UNK> -> words which are not found in the vocab are replace by this token\n",
        "        self.itos = {0: '<PAD>', 1:'<SOS>', 2:'<EOS>', 3: '<UNK>'}\n",
        "        #initiate the token to index dict\n",
        "        self.stoi = {k:j for j,k in self.itos.items()} \n",
        "        \n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.max_size = max_size\n",
        "    \n",
        "    '''\n",
        "    __len__ is used by dataloader later to create batches\n",
        "    '''\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "    \n",
        "    '''\n",
        "    a simple tokenizer to split on space and converts the sentence to list of words\n",
        "    '''\n",
        "    @staticmethod\n",
        "    def tokenizer(text):\n",
        "        return [tok.lower().strip() for tok in text.split(' ')]\n",
        "    \n",
        "    '''\n",
        "    build the vocab: create a dictionary mapping of index to string (itos) and string to index (stoi)\n",
        "    output ex. for stoi -> {'the':5, 'a':6, 'an':7}\n",
        "    '''\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        #calculate the frequencies of each word first to remove the words with freq < freq_threshold\n",
        "        frequencies = {}  #init the freq dict\n",
        "        idx = 4 #index from which we want our dict to start. We already used 4 indexes for pad, start, end, unk\n",
        "        \n",
        "        #calculate freq of words\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer(sentence):\n",
        "                if word not in frequencies.keys():\n",
        "                    frequencies[word]=1\n",
        "                else:\n",
        "                    frequencies[word]+=1\n",
        "                    \n",
        "                    \n",
        "        #limit vocab by removing low freq words\n",
        "        frequencies = {k:v for k,v in frequencies.items() if v>self.freq_threshold} \n",
        "        \n",
        "        #limit vocab to the max_size specified\n",
        "        frequencies = dict(sorted(frequencies.items(), key = lambda x: -x[1])[:self.max_size-idx]) # idx =4 for pad, start, end , unk\n",
        "            \n",
        "        #create vocab\n",
        "        for word in frequencies.keys():\n",
        "            self.stoi[word] = idx\n",
        "            self.itos[idx] = word\n",
        "            idx+=1\n",
        "            \n",
        "    '''\n",
        "    convert the list of words to a list of corresponding indexes\n",
        "    '''    \n",
        "    def numericalize(self, text):\n",
        "        #tokenize text\n",
        "        tokenized_text = self.tokenizer(text)\n",
        "        numericalized_text = []\n",
        "        for token in tokenized_text:\n",
        "            if token in self.stoi.keys():\n",
        "                numericalized_text.append(self.stoi[token])\n",
        "            else: #out-of-vocab words are represented by UNK token index\n",
        "                numericalized_text.append(self.stoi['<UNK>'])\n",
        "                \n",
        "        return numericalized_text"
      ],
      "metadata": {
        "id": "TwUSHYl0hDZ2",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:12:55.469462Z",
          "iopub.execute_input": "2021-12-19T16:12:55.469827Z",
          "iopub.status.idle": "2021-12-19T16:12:55.485899Z",
          "shell.execute_reply.started": "2021-12-19T16:12:55.469784Z",
          "shell.execute_reply": "2021-12-19T16:12:55.484830Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "jtEB7yJbh-Q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class Train_Dataset(Dataset):\n",
        "    '''\n",
        "    Initiating Variables\n",
        "    df: the training dataframe\n",
        "    source_column : the name of source text column in the dataframe\n",
        "    target_columns : the name of target text column in the dataframe\n",
        "    transform : If we want to add any augmentation\n",
        "    freq_threshold : the minimum times a word must occur in corpus to be treated in vocab\n",
        "    source_vocab_max_size : max source vocab size\n",
        "    target_vocab_max_size : max target vocab size\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, df, source_column, target_column, transform=None, freq_threshold = 5,\n",
        "                source_vocab_max_size = 1000000, target_vocab_max_size = 1000000):\n",
        "    \n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        \n",
        "        #get source and target texts\n",
        "        self.source_texts = self.df[source_column]\n",
        "        self.target_texts = self.df[target_column]\n",
        "        \n",
        "        \n",
        "        ##VOCAB class has been created above\n",
        "        #Initialize source vocab object and build vocabulary\n",
        "        self.source_vocab = Vocabulary(freq_threshold, source_vocab_max_size)\n",
        "        self.source_vocab.build_vocabulary(self.source_texts.tolist())\n",
        "        #Initialize target vocab object and build vocabulary\n",
        "        self.target_vocab = Vocabulary(freq_threshold, target_vocab_max_size)\n",
        "        self.target_vocab.build_vocabulary(self.target_texts.tolist())\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    '''\n",
        "    __getitem__ runs on 1 example at a time. Here, we get an example at index and return its numericalize source and\n",
        "    target values using the vocabulary objects we created in __init__\n",
        "    '''\n",
        "    def __getitem__(self, index):\n",
        "        source_text = self.source_texts[index]\n",
        "        target_text = self.target_texts[index]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            source_text = self.transform(source_text)\n",
        "            \n",
        "        #numericalize texts ['<SOS>','cat', 'in', 'a', 'bag','<EOS>'] -> [1,12,2,9,24,2]\n",
        "        numerialized_source = [self.source_vocab.stoi[\"<SOS>\"]]\n",
        "        numerialized_source += self.source_vocab.numericalize(source_text)\n",
        "        numerialized_source.append(self.source_vocab.stoi[\"<EOS>\"])\n",
        "    \n",
        "        numerialized_target = [self.target_vocab.stoi[\"<SOS>\"]]\n",
        "        numerialized_target += self.target_vocab.numericalize(target_text)\n",
        "        numerialized_target.append(self.target_vocab.stoi[\"<EOS>\"])\n",
        "        \n",
        "        #convert the list to tensor and return\n",
        "        return torch.tensor(numerialized_source), torch.tensor(numerialized_target) "
      ],
      "metadata": {
        "id": "h5seXJb0hDk6",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:12:55.488464Z",
          "iopub.execute_input": "2021-12-19T16:12:55.488878Z",
          "iopub.status.idle": "2021-12-19T16:12:55.505266Z",
          "shell.execute_reply.started": "2021-12-19T16:12:55.488836Z",
          "shell.execute_reply": "2021-12-19T16:12:55.502577Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Train_Dataset(train, 'Kor', 'Viet') # ------------------------------------------\n",
        "print(train.loc[1])\n",
        "train_dataset[1]"
      ],
      "metadata": {
        "id": "KYybnVkhhf2m",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:12:55.506668Z",
          "iopub.execute_input": "2021-12-19T16:12:55.506910Z",
          "iopub.status.idle": "2021-12-19T16:12:58.856696Z",
          "shell.execute_reply.started": "2021-12-19T16:12:55.506882Z",
          "shell.execute_reply": "2021-12-19T16:12:58.855645Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dill\n",
        "\n",
        "# Save the file\n",
        "dill.dump(train_dataset, file = open('.//train_dataset_1_Kor_Viet.pickle', \"wb\"))"
      ],
      "metadata": {
        "id": "tZYXWtwpxMs9",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:12:58.858529Z",
          "iopub.execute_input": "2021-12-19T16:12:58.859116Z",
          "iopub.status.idle": "2021-12-19T16:13:05.122792Z",
          "shell.execute_reply.started": "2021-12-19T16:12:58.859073Z",
          "shell.execute_reply": "2021-12-19T16:13:05.121828Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pd.DataFrame(train_dataset.source_vocab.itos.keys()))"
      ],
      "metadata": {
        "id": "6jN254_zwY1P",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:13:05.124266Z",
          "iopub.execute_input": "2021-12-19T16:13:05.124573Z",
          "iopub.status.idle": "2021-12-19T16:13:05.144853Z",
          "shell.execute_reply.started": "2021-12-19T16:13:05.124530Z",
          "shell.execute_reply": "2021-12-19T16:13:05.143802Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pd.DataFrame(train_dataset.target_vocab.itos.keys()))"
      ],
      "metadata": {
        "id": "T3eS0NRJip0E",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:13:05.150030Z",
          "iopub.execute_input": "2021-12-19T16:13:05.150689Z",
          "iopub.status.idle": "2021-12-19T16:13:05.162688Z",
          "shell.execute_reply.started": "2021-12-19T16:13:05.150644Z",
          "shell.execute_reply": "2021-12-19T16:13:05.161527Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Validation_Dataset:\n",
        "    def __init__(self, train_dataset, df, source_column, target_column, transform = None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        \n",
        "        \n",
        "        \n",
        "        #train dataset will be used as lookup for vocab\n",
        "        self.train_dataset = train_dataset\n",
        "        \n",
        "        #get source and target texts\n",
        "        self.source_texts = self.df[source_column]\n",
        "        self.target_texts = self.df[target_column]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        source_text = self.source_texts[index]\n",
        "        #print(source_text)\n",
        "        target_text = self.target_texts[index]\n",
        "        #print(target_text)\n",
        "        if self.transform is not None:\n",
        "            source_text = self.transform(source_text)\n",
        "            \n",
        "        #numericalize texts ['<SOS>','cat', 'in', 'a', 'bag','<EOS>'] -> [1,12,2,9,24,2]\n",
        "        numerialized_source = [self.train_dataset.source_vocab.stoi[\"<SOS>\"]]\n",
        "        numerialized_source += self.train_dataset.source_vocab.numericalize(source_text)\n",
        "        numerialized_source.append(self.train_dataset.source_vocab.stoi[\"<EOS>\"])\n",
        "    \n",
        "        numerialized_target = [self.train_dataset.target_vocab.stoi[\"<SOS>\"]]\n",
        "        numerialized_target += self.train_dataset.target_vocab.numericalize(target_text)\n",
        "        numerialized_target.append(self.train_dataset.target_vocab.stoi[\"<EOS>\"])\n",
        "        #print(numerialized_source)\n",
        "        return torch.tensor(numerialized_source), torch.tensor(numerialized_target) "
      ],
      "metadata": {
        "id": "IFzM1UxsiOC2",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:13:05.164690Z",
          "iopub.execute_input": "2021-12-19T16:13:05.165081Z",
          "iopub.status.idle": "2021-12-19T16:13:05.179364Z",
          "shell.execute_reply.started": "2021-12-19T16:13:05.165036Z",
          "shell.execute_reply": "2021-12-19T16:13:05.178178Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = Validation_Dataset(train_dataset, dev, 'Kor', 'Viet') # -------------------------------------\n",
        "print(dev.loc[1])\n",
        "val_dataset[1]"
      ],
      "metadata": {
        "id": "beZiyYK4jnIL",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:13:05.181382Z",
          "iopub.execute_input": "2021-12-19T16:13:05.182106Z",
          "iopub.status.idle": "2021-12-19T16:13:05.195557Z",
          "shell.execute_reply.started": "2021-12-19T16:13:05.182061Z",
          "shell.execute_reply": "2021-12-19T16:13:05.194264Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCollate:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "        \n",
        "    \n",
        "    #__call__: a default method\n",
        "    ##   First the obj is created using MyCollate(pad_idx) in data loader\n",
        "    ##   Then if obj(batch) is called -> __call__ runs by default\n",
        "    def __call__(self, batch):\n",
        "        #get all source indexed sentences of the batch\n",
        "        source = [item[0] for item in batch] \n",
        "        #pad them using pad_sequence method from pytorch. \n",
        "        source = pad_sequence(source, batch_first=False, padding_value = self.pad_idx) \n",
        "        \n",
        "        #get all target indexed sentences of the batch\n",
        "        target = [item[1] for item in batch] \n",
        "        #pad them using pad_sequence method from pytorch. \n",
        "        target = pad_sequence(target, batch_first=False, padding_value = self.pad_idx)\n",
        "        return source, target"
      ],
      "metadata": {
        "id": "t2y2joThiWbU",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:13:05.197180Z",
          "iopub.execute_input": "2021-12-19T16:13:05.198168Z",
          "iopub.status.idle": "2021-12-19T16:13:05.206698Z",
          "shell.execute_reply.started": "2021-12-19T16:13:05.198109Z",
          "shell.execute_reply": "2021-12-19T16:13:05.205506Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_loader(dataset, batch_size, num_workers=0, shuffle=True, pin_memory=True): #increase num_workers according to CPU\n",
        "    #get pad_idx for collate fn\n",
        "    pad_idx = dataset.source_vocab.stoi['<PAD>']\n",
        "    #define loader\n",
        "    loader = DataLoader(dataset, batch_size = batch_size, num_workers = num_workers,\n",
        "                        shuffle=shuffle,\n",
        "                       pin_memory=pin_memory, collate_fn = MyCollate(pad_idx=pad_idx)) #MyCollate class runs __call__ method by default\n",
        "    return loader\n",
        "\n",
        "def get_valid_loader(dataset, train_dataset, batch_size, num_workers=0, shuffle=True, pin_memory=True):\n",
        "    pad_idx = train_dataset.source_vocab.stoi['<PAD>']\n",
        "    loader = DataLoader(dataset, batch_size = batch_size, num_workers = num_workers,\n",
        "                        shuffle=shuffle,\n",
        "                       pin_memory=pin_memory, collate_fn = MyCollate(pad_idx=pad_idx))\n",
        "    return loader"
      ],
      "metadata": {
        "id": "X9zxyu3SiZgW",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:13:05.208015Z",
          "iopub.execute_input": "2021-12-19T16:13:05.209071Z",
          "iopub.status.idle": "2021-12-19T16:13:05.220994Z",
          "shell.execute_reply.started": "2021-12-19T16:13:05.209031Z",
          "shell.execute_reply": "2021-12-19T16:13:05.220021Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loader = get_train_loader(train_dataset, 32)\n",
        "# source = next(iter(train_loader))[0]\n",
        "# target = next(iter(train_loader))[1]\n",
        "\n",
        "# print('source: \\n', source)\n",
        "\n",
        "# print('source shape: ',source.shape)\n",
        "# print('target shape: ', target.shape)"
      ],
      "metadata": {
        "id": "ibWASw1cib9W",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:13:05.222612Z",
          "iopub.execute_input": "2021-12-19T16:13:05.223561Z",
          "iopub.status.idle": "2021-12-19T16:13:05.232426Z",
          "shell.execute_reply.started": "2021-12-19T16:13:05.223407Z",
          "shell.execute_reply": "2021-12-19T16:13:05.231377Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# val_loader = get_valid_loader(val_dataset, train_dataset, 32)\n",
        "# val_source = next(iter(val_loader))[0]\n",
        "# val_target = next(iter(val_loader))[1]\n",
        "\n",
        "# print('source: \\n', val_source)\n",
        "\n",
        "# print('source shape: ',val_source.shape)\n",
        "# print('target shape: ', val_target.shape)"
      ],
      "metadata": {
        "id": "V4OFwdPkifh8",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:13:05.234120Z",
          "iopub.execute_input": "2021-12-19T16:13:05.234810Z",
          "iopub.status.idle": "2021-12-19T16:13:05.241547Z",
          "shell.execute_reply.started": "2021-12-19T16:13:05.234763Z",
          "shell.execute_reply": "2021-12-19T16:13:05.240586Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### transformer"
      ],
      "metadata": {
        "id": "XCtGDvsacVp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ],
      "metadata": {
        "id": "h9_Md_nOcVqV",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:13:05.243490Z",
          "iopub.execute_input": "2021-12-19T16:13:05.244280Z",
          "iopub.status.idle": "2021-12-19T16:13:05.269150Z",
          "shell.execute_reply.started": "2021-12-19T16:13:05.244239Z",
          "shell.execute_reply": "2021-12-19T16:13:05.268093Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "metadata": {
        "id": "gdeasrWgcVqW",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:13:05.272925Z",
          "iopub.execute_input": "2021-12-19T16:13:05.273832Z",
          "iopub.status.idle": "2021-12-19T16:13:05.284755Z",
          "shell.execute_reply.started": "2021-12-19T16:13:05.273783Z",
          "shell.execute_reply": "2021-12-19T16:13:05.283834Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(train_dataset.source_vocab.itos.keys())\n",
        "TGT_VOCAB_SIZE = len(train_dataset.target_vocab.itos.keys())\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 64 # -------------------------------------------------------------------------------\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "\n",
        "# Define special symbols and indices\n",
        "PAD_IDX, BOS_IDX, EOS_IDX, UNK_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "metadata": {
        "id": "k51v7kykcVqW",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:13:05.286396Z",
          "iopub.execute_input": "2021-12-19T16:13:05.286940Z",
          "iopub.status.idle": "2021-12-19T16:13:14.451750Z",
          "shell.execute_reply.started": "2021-12-19T16:13:05.286896Z",
          "shell.execute_reply": "2021-12-19T16:13:14.450737Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "\n",
        "    train_dataloader = get_train_loader(train_dataset, 64) # change batch_size here -----------------------\n",
        "\n",
        "    for src, tgt in train_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(train_dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_dataloader = get_valid_loader(val_dataset, train_dataset, 64) # change batch_size here -----------------------\n",
        "\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(val_dataloader)\n",
        "#Now we have all the ingredients to train our model. Let’s do it!\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 10 # -----------------------------------------------------------------------------\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "hgr_krwQcVqX",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:13:14.454749Z",
          "iopub.execute_input": "2021-12-19T16:13:14.455095Z",
          "iopub.status.idle": "2021-12-19T16:41:48.481597Z",
          "shell.execute_reply.started": "2021-12-19T16:13:14.455050Z",
          "shell.execute_reply": "2021-12-19T16:41:48.480571Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save model:\n",
        "torch.save(transformer, './/transformer_1_Kor_Viet.pt')\n",
        "\n",
        "#load model:\n",
        "#Define special symbols and indices\n",
        "PAD_IDX, BOS_IDX, EOS_IDX, UNK_IDX = 0, 1, 2, 3\n",
        "model = torch.load('.//transformer_1_Kor_Viet.pt')"
      ],
      "metadata": {
        "id": "E5ry4dqFyKXR",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:41:48.483162Z",
          "iopub.execute_input": "2021-12-19T16:41:48.485693Z",
          "iopub.status.idle": "2021-12-19T16:41:49.094509Z",
          "shell.execute_reply.started": "2021-12-19T16:41:48.485651Z",
          "shell.execute_reply": "2021-12-19T16:41:49.093538Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    sentence = src_sentence.split(' ')\n",
        "    index_sentence = []\n",
        "    for sent in sentence:\n",
        "      try:\n",
        "        index_sentence.append(train_dataset.source_vocab.stoi[sent])\n",
        "      except:\n",
        "        index_sentence.append(3)\n",
        "    tensor_sentence = torch.tensor(index_sentence)\n",
        "\n",
        "    src = tensor_sentence.view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "\n",
        "    sent = []\n",
        "    for inx in tgt_tokens:\n",
        "      sent.append(train_dataset.target_vocab.itos[int(inx)])\n",
        "    sent = ' '.join(sent)\n",
        "    return sent.replace(\"<SOS>\", \"\").replace(\"<EOS>\", \"\")"
      ],
      "metadata": {
        "id": "Yxdf9B40fjdR",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:41:49.100161Z",
          "iopub.execute_input": "2021-12-19T16:41:49.102769Z",
          "iopub.status.idle": "2021-12-19T16:41:49.124991Z",
          "shell.execute_reply.started": "2021-12-19T16:41:49.102724Z",
          "shell.execute_reply": "2021-12-19T16:41:49.124041Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test_pred = []\n",
        "for i in range(len(test)):\n",
        "  Y_test_pred.append(translate(model, test['Kor'][i]))\n"
      ],
      "metadata": {
        "id": "PKFJR5Ykydx4",
        "execution": {
          "iopub.status.busy": "2021-12-19T16:41:49.131228Z",
          "iopub.execute_input": "2021-12-19T16:41:49.134164Z",
          "iopub.status.idle": "2021-12-19T17:13:06.861920Z",
          "shell.execute_reply.started": "2021-12-19T16:41:49.134116Z",
          "shell.execute_reply": "2021-12-19T17:13:06.860955Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['predicted_test'] = Y_test_pred\n",
        "test.to_csv('./predicted_test_Kor_Viet.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-19T17:13:06.864353Z",
          "iopub.execute_input": "2021-12-19T17:13:06.864723Z",
          "iopub.status.idle": "2021-12-19T17:13:07.230557Z",
          "shell.execute_reply.started": "2021-12-19T17:13:06.864661Z",
          "shell.execute_reply": "2021-12-19T17:13:07.229479Z"
        },
        "trusted": true,
        "id": "k_tI32GMftTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "print('Bleu Score: ', corpus_bleu(test['Viet'], Y_test_pred, smoothing_function=SmoothingFunction().method4))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-19T17:13:07.232297Z",
          "iopub.execute_input": "2021-12-19T17:13:07.232660Z",
          "iopub.status.idle": "2021-12-19T17:15:52.831182Z",
          "shell.execute_reply.started": "2021-12-19T17:13:07.232617Z",
          "shell.execute_reply": "2021-12-19T17:15:52.830221Z"
        },
        "trusted": true,
        "id": "ZtCNZDnfftTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.translate.gleu_score as gleu\n",
        "print('Gleu Score: ', gleu.corpus_gleu([[i] for i in test['Viet'].tolist()], Y_test_pred, min_len=1, max_len=4))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-19T17:15:52.832735Z",
          "iopub.execute_input": "2021-12-19T17:15:52.833511Z",
          "iopub.status.idle": "2021-12-19T17:16:02.003881Z",
          "shell.execute_reply.started": "2021-12-19T17:15:52.833450Z",
          "shell.execute_reply": "2021-12-19T17:16:02.001761Z"
        },
        "trusted": true,
        "id": "cbgnXA7IftTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-19T17:16:02.005908Z",
          "iopub.execute_input": "2021-12-19T17:16:02.006293Z",
          "iopub.status.idle": "2021-12-19T17:16:12.161705Z",
          "shell.execute_reply.started": "2021-12-19T17:16:02.006250Z",
          "shell.execute_reply": "2021-12-19T17:16:12.160599Z"
        },
        "trusted": true,
        "id": "Rra8EKZfftTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jiwer import wer\n",
        "\n",
        "error = wer(test['Viet'].tolist(), Y_test_pred)\n",
        "error"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-19T17:16:12.168362Z",
          "iopub.execute_input": "2021-12-19T17:16:12.168635Z",
          "iopub.status.idle": "2021-12-19T17:16:13.492408Z",
          "shell.execute_reply.started": "2021-12-19T17:16:12.168601Z",
          "shell.execute_reply": "2021-12-19T17:16:13.491482Z"
        },
        "trusted": true,
        "id": "J-mh7KYEftTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TER:\n",
        "import itertools as itrt\n",
        "\n",
        "\n",
        "def ter(inputwords, refwords):\n",
        "    \"\"\"Calcurate Translation Error Rate\n",
        "    inputwords and refwords are both list object.\n",
        "    >>> ref = 'SAUDI ARABIA denied THIS WEEK information published in the AMERICAN new york times'.split()\n",
        "    >>> hyp = 'THIS WEEK THE SAUDIS denied information published in the new york times'.split()\n",
        "    >>> '{0:.3f}'.format(ter(hyp, ref))\n",
        "    '0.308'\n",
        "    \"\"\"\n",
        "    inputwords, refwords = list(inputwords), list(refwords)\n",
        "    ed = CachedEditDistance(refwords)\n",
        "    return _ter(inputwords, refwords, ed)\n",
        "\n",
        "\n",
        "def _ter(iwords, rwords, mtd):\n",
        "    \"\"\" Translation Edit Rate core function \"\"\"\n",
        "    err = 0\n",
        "    while True:\n",
        "        delta, new_iwords = _shift(iwords, rwords, mtd)\n",
        "        if delta <= 0:\n",
        "            break\n",
        "        err += 1\n",
        "        iwords = new_iwords\n",
        "    return (err + mtd(iwords)) / len(rwords)\n",
        "\n",
        "\n",
        "def _shift(iwords, rwords, mtd):\n",
        "    \"\"\" Shift the phrase pair most reduce the edit_distance\n",
        "    Return True if shift occurred, else False.\n",
        "    \"\"\"\n",
        "    pre_score = mtd(iwords)\n",
        "    scores = []\n",
        "    for isp, rsp, length in _findpairs(iwords, rwords):\n",
        "        shifted_words = iwords[:isp] + iwords[isp + length:]\n",
        "        shifted_words[rsp:rsp] = iwords[isp:isp + length]\n",
        "        scores.append((pre_score - mtd(shifted_words), shifted_words))\n",
        "\n",
        "    if not scores:\n",
        "        return 0, iwords\n",
        "\n",
        "    scores.sort()\n",
        "    return scores[-1]\n",
        "\n",
        "\n",
        "def _findpairs(ws1, ws2):\n",
        "    \"\"\" yield the tuple of (ws1_start_point, ws2_start_point, length)\n",
        "    So ws1[ws1_start_point:ws1_start_point+length] == ws2[ws2_start_point:ws2_start_point+length]\n",
        "    \"\"\"\n",
        "    for i1, i2 in itrt.product(range(len(ws1)), range(len(ws2))):\n",
        "        if i1 == i2:\n",
        "            continue\n",
        "        if ws1[i1] == ws2[i2]:\n",
        "            length = 1\n",
        "            for j1, j2 in zip(range(i1 + 1, len(ws1)), range(i2 + 1, len(ws2))):\n",
        "                if ws1[j1] == ws2[j2]:\n",
        "                    length += 1\n",
        "                else:\n",
        "                    break\n",
        "            yield (i1, i2, length)\n",
        "\n",
        "\n",
        "def _gen_matrix(col_size, row_size, default=None):\n",
        "    return [[default for _ in range(row_size)] for __ in range(col_size)]\n",
        "\n",
        "\n",
        "def edit_distance(s, t):\n",
        "    \"\"\" Levenshtein distance\"\"\"\n",
        "    l = _gen_matrix(len(s) + 1, len(t) + 1, None)\n",
        "    l[0] = [x for x, _ in enumerate(l[0])]\n",
        "    for x, y in enumerate(l):\n",
        "        y[0] = x\n",
        "    for i, j in itrt.product(range(1, len(s) + 1), range(1, len(t) + 1)):\n",
        "        l[i][j] = min(l[i - 1][j] + 1,\n",
        "                      l[i][j - 1] + 1,\n",
        "                      l[i - 1][j - 1] + (0 if s[i - 1] == t[j - 1] else 1))\n",
        "    return l[-1][-1]\n",
        "\n",
        "\n",
        "class CachedEditDistance(object):\n",
        "    def __init__(self, rwords):\n",
        "        self.rwds = rwords\n",
        "        self._cache = {}\n",
        "        self.list_for_copy = [0 for _ in range(len(self.rwds) + 1)]\n",
        "\n",
        "    def __call__(self, iwords):\n",
        "        start_position, cached_score = self._find_cache(iwords)\n",
        "        score, newly_created_matrix = self._edit_distance(iwords, start_position, cached_score)\n",
        "        self._add_cache(iwords, newly_created_matrix)\n",
        "        return score\n",
        "\n",
        "    def _edit_distance(self, iwords, spos, cache):\n",
        "        if cache is None:\n",
        "            cache = [tuple(range(len(self.rwds) + 1))]\n",
        "        else:\n",
        "            cache = [cache]\n",
        "\n",
        "        l = cache + [list(self.list_for_copy) for _ in range(len(iwords) - spos)]\n",
        "\n",
        "        assert len(l) - 1 == len(iwords) - spos\n",
        "\n",
        "        for i, j in itrt.product(range(1, len(iwords) - spos + 1), range(len(self.rwds) + 1)):\n",
        "            if j == 0:\n",
        "                l[i][j] = l[i - 1][j] + 1\n",
        "            else:\n",
        "                l[i][j] = min(l[i - 1][j] + 1,\n",
        "                              l[i][j - 1] + 1,\n",
        "                              l[i - 1][j - 1] + (0 if iwords[spos + i - 1] == self.rwds[j - 1] else 1))\n",
        "        return l[-1][-1], l[1:]\n",
        "\n",
        "    def _add_cache(self, iwords, mat):\n",
        "        node = self._cache\n",
        "        skipnum = len(iwords) - len(mat)\n",
        "        for i in range(skipnum):\n",
        "            node = node[iwords[i]][0]\n",
        "        assert len(iwords[skipnum:]) == len(mat)\n",
        "        for word, row in zip(iwords[skipnum:], mat):\n",
        "            if word not in node:\n",
        "                node[word] = [{}, None]\n",
        "            value = node[word]\n",
        "            if value[1] is None:\n",
        "                value[1] = tuple(row)\n",
        "            node = value[0]\n",
        "\n",
        "    def _find_cache(self, iwords):\n",
        "        node = self._cache\n",
        "        start_position, row = 0, None\n",
        "        for idx, word in enumerate(iwords):\n",
        "            if word in node:\n",
        "                start_position = idx + 1\n",
        "                node, row = node[word]\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return start_position, row"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-19T17:16:13.494110Z",
          "iopub.execute_input": "2021-12-19T17:16:13.494609Z",
          "iopub.status.idle": "2021-12-19T17:16:13.527130Z",
          "shell.execute_reply.started": "2021-12-19T17:16:13.494561Z",
          "shell.execute_reply": "2021-12-19T17:16:13.525981Z"
        },
        "trusted": true,
        "id": "D4flU324ftTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum_ter = 0\n",
        "for i in range(len(test)):\n",
        "    hyp = Y_test_pred[i].split()\n",
        "    ref = test['Viet'][i].split()\n",
        "    sum_ter += ter(hyp, ref)\n",
        "TER = sum_ter/len(test)\n",
        "print('TER: ', TER)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-19T17:16:13.529336Z",
          "iopub.execute_input": "2021-12-19T17:16:13.530090Z",
          "iopub.status.idle": "2021-12-19T17:17:00.054836Z",
          "shell.execute_reply.started": "2021-12-19T17:16:13.530046Z",
          "shell.execute_reply": "2021-12-19T17:17:00.052697Z"
        },
        "trusted": true,
        "id": "ptj7Ogr5ftTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Viet - Kor\n",
        "https://pytorch.org/tutorials/beginner/translation_transformer.html"
      ],
      "metadata": {
        "id": "5VpHzHSZvUG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare dataset to Dataloader:\n",
        "https://towardsdatascience.com/custom-datasets-in-pytorch-part-2-text-machine-translation-71c41a3e994e"
      ],
      "metadata": {
        "id": "vIOuccXTvUHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Train_Dataset(train, 'Viet', 'Kor') # ------------------------------------------\n",
        "print(train.loc[1])\n",
        "train_dataset[1]"
      ],
      "metadata": {
        "id": "P79j3eSYvUHp",
        "execution": {
          "iopub.status.busy": "2021-12-19T17:17:00.056707Z",
          "iopub.execute_input": "2021-12-19T17:17:00.056981Z",
          "iopub.status.idle": "2021-12-19T17:17:03.290048Z",
          "shell.execute_reply.started": "2021-12-19T17:17:00.056942Z",
          "shell.execute_reply": "2021-12-19T17:17:03.289059Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dill\n",
        "\n",
        "# Save the file\n",
        "dill.dump(train_dataset, file = open('./train_dataset_1_Viet_Kor.pickle', \"wb\"))"
      ],
      "metadata": {
        "id": "EKnKMvckw-Ft",
        "execution": {
          "iopub.status.busy": "2021-12-19T17:17:03.291846Z",
          "iopub.execute_input": "2021-12-19T17:17:03.292453Z",
          "iopub.status.idle": "2021-12-19T17:17:09.288784Z",
          "shell.execute_reply.started": "2021-12-19T17:17:03.292412Z",
          "shell.execute_reply": "2021-12-19T17:17:09.287789Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = Validation_Dataset(train_dataset, dev, 'Viet', 'Kor') # -------------------------------------\n",
        "print(dev.loc[1])\n",
        "val_dataset[1]"
      ],
      "metadata": {
        "id": "csiHgR1DvUHr",
        "execution": {
          "iopub.status.busy": "2021-12-19T17:17:09.290474Z",
          "iopub.execute_input": "2021-12-19T17:17:09.290799Z",
          "iopub.status.idle": "2021-12-19T17:17:09.307148Z",
          "shell.execute_reply.started": "2021-12-19T17:17:09.290760Z",
          "shell.execute_reply": "2021-12-19T17:17:09.306168Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### transformer"
      ],
      "metadata": {
        "id": "a6biYr6HvUHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(train_dataset.source_vocab.itos.keys())\n",
        "TGT_VOCAB_SIZE = len(train_dataset.target_vocab.itos.keys())\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 64 # -------------------------------------------------------------------------------\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "\n",
        "# Define special symbols and indices\n",
        "PAD_IDX, BOS_IDX, EOS_IDX, UNK_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "metadata": {
        "id": "PdDll93LvUHu",
        "execution": {
          "iopub.status.busy": "2021-12-19T17:17:09.308905Z",
          "iopub.execute_input": "2021-12-19T17:17:09.310742Z",
          "iopub.status.idle": "2021-12-19T17:17:10.327892Z",
          "shell.execute_reply.started": "2021-12-19T17:17:09.310698Z",
          "shell.execute_reply": "2021-12-19T17:17:10.326717Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 10 # -----------------------------------------------------------------------------\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
      ],
      "metadata": {
        "id": "M5uV2aMJvUHu",
        "execution": {
          "iopub.status.busy": "2021-12-19T17:17:10.330052Z",
          "iopub.execute_input": "2021-12-19T17:17:10.330360Z",
          "iopub.status.idle": "2021-12-19T17:49:02.430994Z",
          "shell.execute_reply.started": "2021-12-19T17:17:10.330317Z",
          "shell.execute_reply": "2021-12-19T17:49:02.430064Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save model:\n",
        "torch.save(transformer, './transformer_1_Viet_Kor.pt')\n",
        "\n",
        "#load model:\n",
        "#Define special symbols and indices\n",
        "PAD_IDX, BOS_IDX, EOS_IDX, UNK_IDX = 0, 1, 2, 3\n",
        "model = torch.load('./transformer_1_Viet_Kor.pt')"
      ],
      "metadata": {
        "id": "IeSK7uMNvUHv",
        "execution": {
          "iopub.status.busy": "2021-12-19T17:49:02.433159Z",
          "iopub.execute_input": "2021-12-19T17:49:02.433504Z",
          "iopub.status.idle": "2021-12-19T17:49:03.021359Z",
          "shell.execute_reply.started": "2021-12-19T17:49:02.433463Z",
          "shell.execute_reply": "2021-12-19T17:49:03.020427Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict test"
      ],
      "metadata": {
        "id": "3zJIIOUaftTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test_pred = []\n",
        "for i in range(len(test)):\n",
        "  Y_test_pred.append(translate(model, test['Viet'][i]))\n"
      ],
      "metadata": {
        "id": "phznnpIsvUHw",
        "execution": {
          "iopub.status.busy": "2021-12-19T17:49:03.022889Z",
          "iopub.execute_input": "2021-12-19T17:49:03.023228Z",
          "iopub.status.idle": "2021-12-19T18:11:49.738056Z",
          "shell.execute_reply.started": "2021-12-19T17:49:03.023188Z",
          "shell.execute_reply": "2021-12-19T18:11:49.737055Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['predicted_test'] = Y_test_pred\n",
        "test.to_csv('./predicted_test_Viet_Kor.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-19T18:11:49.739704Z",
          "iopub.execute_input": "2021-12-19T18:11:49.741045Z",
          "iopub.status.idle": "2021-12-19T18:11:50.124126Z",
          "shell.execute_reply.started": "2021-12-19T18:11:49.740997Z",
          "shell.execute_reply": "2021-12-19T18:11:50.123078Z"
        },
        "trusted": true,
        "id": "bs6Sh_pAftTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "print('Bleu Score: ', corpus_bleu(test['Kor'], Y_test_pred, smoothing_function=SmoothingFunction().method4))\n",
        "\n",
        "import nltk.translate.gleu_score as gleu\n",
        "print('Gleu Score: ', gleu.corpus_gleu([[i] for i in test['Kor'].tolist()], Y_test_pred, min_len=1, max_len=4))\n",
        "\n",
        "from jiwer import wer\n",
        "\n",
        "error = wer(test['Kor'].tolist(), Y_test_pred)\n",
        "print('WER: ', error)\n",
        "\n",
        "sum_ter = 0\n",
        "for i in range(len(test)):\n",
        "    hyp = Y_test_pred[i].split()\n",
        "    ref = test['Kor'][i].split()\n",
        "    sum_ter += ter(hyp, ref)\n",
        "TER = sum_ter/len(test)\n",
        "print('TER: ', TER)"
      ],
      "metadata": {
        "id": "YwxYZGN-vUHw",
        "execution": {
          "iopub.status.busy": "2021-12-19T18:11:50.125729Z",
          "iopub.execute_input": "2021-12-19T18:11:50.126928Z",
          "iopub.status.idle": "2021-12-19T18:12:50.937155Z",
          "shell.execute_reply.started": "2021-12-19T18:11:50.126886Z",
          "shell.execute_reply": "2021-12-19T18:12:50.935222Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O82FrxUxftTz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}